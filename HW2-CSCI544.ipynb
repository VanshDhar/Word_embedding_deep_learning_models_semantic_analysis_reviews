{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31b88654",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32eee78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "! pip install bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "#from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC as SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from numpy import argmax\n",
    "from copy import deepcopy\n",
    "from numpy import vstack\n",
    "\n",
    "! pip install gensim\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "import gensim.models\n",
    "from gensim import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c086e3cd",
   "metadata": {},
   "source": [
    "# Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee5716",
   "metadata": {},
   "source": [
    "### 1. Dataset Generation (5 points)\n",
    "### We will use the Amazon reviews dataset used in HW1. Load the dataset and build a balanced dataset of 250K reviews along with their ratings (50K instances per each rating score) through random selection. Create ternary labels using the ratings. We assume that ratings more than 3 denote positive 1 sentiment (class 1) and rating less than 3 denote negative sentiment (class 2). Reviews with rating 3 are considered to have neutral sentiment (class 3). You can store your dataset after generation and reuse it to reduce the computational load. For your experiments consider a 80%/20% training/testing split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f75dac",
   "metadata": {},
   "source": [
    "### Ans: Loading the dataset in the cell below, by first reading the tsv file and then removing any row of data having inconsistent value, I also drop all the columns of the dataset except the reviw_body and the star rating, since these are the only coulmns of use to us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e2c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dataset\n",
    "file_path = 'amazon_reviews_us_Kitchen_v1_00.tsv'\n",
    "input_data = pd.read_csv(file_path, sep='\\t',error_bad_lines=False,warn_bad_lines=False)\n",
    "input_data =input_data.dropna()\n",
    "\n",
    "input_data = input_data[['review_body','star_rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406c6faf",
   "metadata": {},
   "source": [
    "### Ans: Created a balanced dataset in below cell by taking 50000 reviews for each rating score randomly and merging them together.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8fc8af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating ternary reviews\n",
    "review_1_data = input_data.loc[ input_data['star_rating'] == 1.0 ]\n",
    "review_1_data = review_1_data.sample(n=50000, random_state=65)\n",
    "review_2_data = input_data.loc[ input_data['star_rating'] == 2.0 ]\n",
    "review_2_data = review_2_data.sample(n=50000, random_state=65)\n",
    "review_3_data = input_data.loc[ input_data['star_rating'] == 3.0 ]\n",
    "review_3_data = review_3_data.sample(n=50000, random_state=65)\n",
    "review_4_data = input_data.loc[ input_data['star_rating'] == 4.0 ]\n",
    "review_4_data = review_4_data.sample(n=50000, random_state=65)\n",
    "review_5_data = input_data.loc[ input_data['star_rating'] == 5.0 ]\n",
    "review_5_data = review_5_data.sample(n=50000, random_state=65)\n",
    "\n",
    "input_data = pd.concat([review_1_data,review_2_data,review_3_data,review_4_data,review_5_data])\n",
    "input_data = input_data.sample(frac = 1, random_state=65).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a38a892",
   "metadata": {},
   "source": [
    "### Ans: In the below cell, assigned each row of data a ternary label based on their star rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cfc534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating ternary labels\n",
    "input_data['ternary_label'] = input_data['star_rating']\n",
    "\n",
    "\n",
    "input_data.loc[input_data['star_rating'] > 3.0, 'ternary_label'] = 1\n",
    "input_data.loc[input_data['star_rating'] < 3.0, 'ternary_label'] = 2\n",
    "input_data.loc[input_data['star_rating'] == 3.0, 'ternary_label'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58843212",
   "metadata": {},
   "source": [
    "### Ans: Randomly splitting the dataset in the cell below into training and testing dataset with 80%/20% split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1023170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset into training and testing for ternary reviews\n",
    "training_dataset = input_data.sample(frac = 0.8, random_state=65)\n",
    "testing_dataset = input_data.drop(training_dataset.index)\n",
    "training_dataset = training_dataset.reset_index(drop=True)\n",
    "testing_dataset = testing_dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e5454",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37fbc0c",
   "metadata": {},
   "source": [
    "### Performing data cleaning below on training and testing dataset, similar to HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71ffcb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting reviews in lower case \n",
    "training_dataset['review_body'] = training_dataset['review_body'].str.lower()\n",
    "testing_dataset['review_body'] = testing_dataset['review_body'].str.lower()\n",
    "\n",
    "#removing HTML tags\n",
    "training_dataset['review_body'] = training_dataset['review_body'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "testing_dataset['review_body'] = testing_dataset['review_body'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "\n",
    "#removing URL tags\n",
    "training_dataset['review_body'] = training_dataset['review_body'].apply(lambda x: re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x))\n",
    "testing_dataset['review_body'] = testing_dataset['review_body'].apply(lambda x: re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x))\n",
    "\n",
    "#removing non-alphabetical characters\n",
    "training_dataset['review_body'] = training_dataset['review_body'].str.replace('[^a-zA-Z ]', ' ')\n",
    "testing_dataset['review_body'] = testing_dataset['review_body'].str.replace('[^a-zA-Z ]', ' ')\n",
    "\n",
    "#removing the extra spaces between words\n",
    "training_dataset['review_body'] = training_dataset['review_body'].apply(lambda x: re.sub(' +', ' ', x))\n",
    "testing_dataset['review_body'] = testing_dataset['review_body'].apply(lambda x: re.sub(' +', ' ', x))\n",
    "\n",
    "#performing contractions\n",
    "def Contractionfunction(text):\n",
    "    expanded_words = []     \n",
    "    for word in text.split(): \n",
    "        expanded_words.append(contractions.fix(word))    \n",
    "\n",
    "    expanded_text = ' '.join(expanded_words)\n",
    "    return expanded_text\n",
    "\n",
    "training_dataset['review_body'] = training_dataset['review_body'].apply(lambda x: Contractionfunction(x))\n",
    "testing_dataset['review_body'] = testing_dataset['review_body'].apply(lambda x: Contractionfunction(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25ecbdd",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb60ebc",
   "metadata": {},
   "source": [
    "### Performing Pre-Processing below on training and testing dataset, similar to HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e757307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words\n",
    "StopWords = stopwords.words('english')\n",
    "\n",
    "training_dataset['review_body'] = training_dataset['review_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in (StopWords)]))\n",
    "testing_dataset['review_body'] = testing_dataset['review_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in (StopWords)]))\n",
    "\n",
    "#performing lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatize_tokens = [lemmatizer.lemmatize(w) for w in word_tokenize(text)]\n",
    "    return ' '.join(lemmatize_tokens)\n",
    "    \n",
    "training_dataset['review_body'] = training_dataset['review_body'].apply(lambda x: lemmatize_text(x))\n",
    "testing_dataset['review_body'] = testing_dataset['review_body'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859341eb",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba5e103",
   "metadata": {},
   "source": [
    "### 2. Word Embedding (30 points)\n",
    "### In this part the of the assignment, you will learn how to generate two sets of Word2Vec features for the dataset you generated. You can use Gensim library for this purpose. A helpful tutorial is available in the following link:\n",
    "   https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "### (a) (10 points)\n",
    "### Load the pretrained “word2vec-google-news-300” Word2Vec model and learn how to extract word embeddings for your dataset. Try to check semantic similarities of the generated vectors using two examples of your own, e.g., King − Man + Woman = Queen or excellent ∼ outstanding.\n",
    "### (b) (20 points)\n",
    "### Train a Word2Vec model using your own dataset. Set the embedding size to be 300 and the window size to be 11. You can also consider a minimum word count of 10. Check the semantic similarities for the same two examples in part (a). What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d366bcb2",
   "metadata": {},
   "source": [
    "## Pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b619b25c",
   "metadata": {},
   "source": [
    "### Part (a) Ans: In the cell below we load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f97875b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the pretrained word2vec model\n",
    "word_vec_pretrain_model = api.load('word2vec-google-news-300')\n",
    "#print(word_vec_pretrain_model['Dinner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624ffed0",
   "metadata": {},
   "source": [
    "### Part (a) Ans: In the cell below we check semantic similarities of the pretrained model on example:\n",
    "### 1) Father - Boy + Mother = ?\n",
    "### 2) Tea ~ Coffee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "46f6c33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Pre-trained Model: Father - Boy + Mother = husband, with a cosine similarity of 0.7670101523399353\n",
      "For Pre-trained Model: Tea and Coffee have a cosine similarity of 0.5635292530059814\n"
     ]
    }
   ],
   "source": [
    "print(\"For Pre-trained Model: Father - Boy + Mother = {}, with a cosine similarity of {}\".format(word_vec_pretrain_model.most_similar(positive=[\"father\", \"mother\"], negative=[\"boy\"], topn=1)[0][0],word_vec_pretrain_model.most_similar(positive=[\"father\", \"mother\"], negative=[\"boy\"], topn=1)[0][1]))\n",
    "print(\"For Pre-trained Model: Tea and Coffee have a cosine similarity of {}\".format(word_vec_pretrain_model.similarity(\"tea\", \"coffee\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399101a",
   "metadata": {},
   "source": [
    "## Our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab38398",
   "metadata": {},
   "source": [
    "### Part (b) Ans: In the cell below we train a Word2Vec model using our own dataset and set the embedding size to be 300, the window size to be 11 and a minimum word count of 10. We also create a class MyCorpus to iteratively feed each review of the training dataset to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10107ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the Word2Vec model on our own dataset\n",
    "\n",
    "class MyCorpus(object):\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        for line in training_dataset['review_body'].tolist():\n",
    "            yield utils.simple_preprocess(line)\n",
    "            \n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences,vector_size=300,window=11,min_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7127def0",
   "metadata": {},
   "source": [
    "### Part (b) Ans: In the cell below we check semantic similarities of our model on example:\n",
    "### 1) Father - Boy + Mother = ?\n",
    "### 2) Tea ~ Coffee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f9f30dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Our Model: Father - Boy + Mother = sister, with a cosine similarity of 0.6096693873405457\n",
      "For Our Model: Tea and Coffee have a cosine similarity of 0.5299463868141174\n"
     ]
    }
   ],
   "source": [
    "print(\"For Our Model: Father - Boy + Mother = {}, with a cosine similarity of {}\".format(model.wv.most_similar(positive=[\"father\", \"mother\"], negative=[\"boy\"], topn=1)[0][0],model.wv.most_similar(positive=[\"father\", \"mother\"], negative=[\"boy\"], topn=1)[0][1]))\n",
    "print(\"For Our Model: Tea and Coffee have a cosine similarity of {}\".format(model.wv.similarity(\"tea\", \"coffee\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a53606",
   "metadata": {},
   "source": [
    "### Part (b) Ans: From the above results based on cosine similarity for 2nd example pretrained model encodes semantic similarity better, but the results for the 1st example indicate, intuitively our model gave the more logical result although with a lower cosine similarity, so overall no one model seems to be clearly better than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b65778",
   "metadata": {},
   "source": [
    "### In the cell below we create a binary training and testing dataset for class 1 and 2,  (denoted by '_simplified'), we also write 2 functions to generate the word2vec feature vectors for each review in the dataset, one for our model and the other for pretrained model (genVec and genVec_pretrained respectively), these functions return the input(reviews) in the various format, that they will be required, to train the upcoming models( i.e, average word2vec vector, first 10 word2vec vector, first 50 word2vec vector  representations for each review). We then store the output of these functions in appropriate variables to use in the code further below to train and test our various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6bfcb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating binary dataset\n",
    "training_dataset_simplified = training_dataset.loc[ training_dataset['star_rating'] != 3.0 ]\n",
    "testing_dataset_simplified = testing_dataset.loc[ testing_dataset['star_rating'] != 3.0 ]\n",
    "training_dataset_simplified = training_dataset_simplified.reset_index(drop=True)\n",
    "testing_dataset_simplified = testing_dataset_simplified.reset_index(drop=True)\n",
    "\n",
    "#functions for converting input to various word2vec vector representations\n",
    "def genVec(reviews,rating_class,model):\n",
    "    X = []\n",
    "    y = [] \n",
    "    X_10 = []\n",
    "    y_10 = [] \n",
    "    X_50 = []\n",
    "    y_50 = []\n",
    "    for i in range(len(reviews)):\n",
    "        try:\n",
    "            wordVecs = [model.wv[w] for w in reviews[i].split() if w in model.wv.index_to_key]\n",
    "            if len(wordVecs) > 0:\n",
    "                senVec = np.mean([ele for ele in wordVecs], axis=0).tolist()\n",
    "                X.append(senVec)\n",
    "                y.append(rating_class[i])\n",
    "            if len(wordVecs) >= 10:\n",
    "                X_10.append(wordVecs[:10])\n",
    "                y_10.append(rating_class[i])\n",
    "            if len(wordVecs) >= 50:\n",
    "                X_50.append(wordVecs[:50])\n",
    "                y_50.append(rating_class[i])\n",
    "            elif len(wordVecs) > 0:\n",
    "                X_50.append(wordVecs)\n",
    "                y_50.append(rating_class[i])\n",
    "        except:\n",
    "            pass\n",
    "    return X, y, X_10, y_10, X_50, y_50\n",
    "\n",
    "def genVec_pretrained(reviews,rating_class,model):\n",
    "    X = []\n",
    "    y = [] \n",
    "    X_10 = []\n",
    "    y_10 = [] \n",
    "    X_50 = []\n",
    "    y_50 = []\n",
    "    for i in range(len(reviews)):\n",
    "        try:\n",
    "            wordVecs = [model[w] for w in reviews[i].split() if w in model]\n",
    "            if len(wordVecs) > 0:\n",
    "                senVec = np.mean([ele for ele in wordVecs], axis=0).tolist()\n",
    "                X.append(senVec)\n",
    "                y.append(rating_class[i])\n",
    "            if len(wordVecs) >= 10:\n",
    "                X_10.append(wordVecs[:10])\n",
    "                y_10.append(rating_class[i])\n",
    "            if len(wordVecs) >= 50:\n",
    "                X_50.append(wordVecs[:50])\n",
    "                y_50.append(rating_class[i])\n",
    "            elif len(wordVecs) > 0:\n",
    "                \n",
    "                X_50.append(wordVecs)\n",
    "                y_50.append(rating_class[i])\n",
    "        except:\n",
    "            pass\n",
    "    return X, y, X_10, y_10, X_50, y_50\n",
    "\n",
    "#storing the various inputs for training and testing, that will be feed to the upcoming models\n",
    "X_train_binary, y_train_binary, X_train_binary_10, y_train_binary_10, X_train_binary_50, y_train_binary_50 = genVec(training_dataset_simplified['review_body'], training_dataset_simplified['ternary_label'],model)\n",
    "X_test_binary, y_test_binary, X_test_binary_10, y_test_binary_10, X_test_binary_50, y_test_binary_50 = genVec(testing_dataset_simplified['review_body'], testing_dataset_simplified['ternary_label'],model)\n",
    "\n",
    "X_train_binary_pre, y_train_binary_pre, X_train_binary_pre_10, y_train_binary_pre_10, X_train_binary_pre_50, y_train_binary_pre_50 = genVec_pretrained(training_dataset_simplified['review_body'], training_dataset_simplified['ternary_label'],word_vec_pretrain_model)\n",
    "X_test_binary_pre, y_test_binary_pre, X_test_binary_pre_10, y_test_binary_pre_10, X_test_binary_pre_50, y_test_binary_pre_50 = genVec_pretrained(testing_dataset_simplified['review_body'], testing_dataset_simplified['ternary_label'],word_vec_pretrain_model)\n",
    "\n",
    "X_train, y_train, X_train_10, y_train_10, X_train_50, y_train_50 = genVec(training_dataset['review_body'], training_dataset['ternary_label'],model)\n",
    "X_test, y_test, X_test_10, y_test_10, X_test_50, y_test_50 = genVec(testing_dataset['review_body'], testing_dataset['ternary_label'],model)\n",
    "\n",
    "X_train_pre, y_train_pre, X_train_pre_10, y_train_pre_10, X_train_pre_50, y_train_pre_50 = genVec_pretrained(training_dataset['review_body'], training_dataset['ternary_label'],word_vec_pretrain_model)\n",
    "X_test_pre, y_test_pre, X_test_pre_10, y_test_pre_10, X_test_pre_50, y_test_pre_50 = genVec_pretrained(testing_dataset['review_body'], testing_dataset['ternary_label'],word_vec_pretrain_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab252368",
   "metadata": {},
   "source": [
    "# Simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f1e0e4",
   "metadata": {},
   "source": [
    "### 3. Simple models (20 points)\n",
    "### Using the Word2Vec features that you can generate using the two models you prepared in the Word Embedding section, train a perceptron and an SVM model similar to HW1 for class 1 and class 2 (binary models). For this purpose, you can just use the average Word2Vec vectors for each review as the input feature . To improve your performance, use the data cleaning and preprocessing steps of HW1 to include only important words from each review when you compute the average. Report your accuracy values on the testing split for these models for each feature type along with values you reported in your HW1 submission, i.e., for each of perceptron and SVM, you need to report three accuracy values for “word2vec-google-news-300”, your own Word2Vec, and TF-IDF features. What do you conclude from comparing performances for the models trained using the three different feature types (TF-IDF, pretrained Word2Vec, your trained Word2Vec)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc2334",
   "metadata": {},
   "source": [
    "### The function below calculates the accuracy, precision, recall and fscore given the predicted and actual output list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "364228df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_aprf(name,model,y,y_pred):\n",
    "    print('For {} model, {} has accuracy: {}, precision: {}, recall: {}, f-score: {}'.format(model,name,accuracy_score(y, y_pred),precision_score(y, y_pred),recall_score(y, y_pred),f1_score(y, y_pred)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e65bb33",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262828d8",
   "metadata": {},
   "source": [
    "## Our word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c3f5aa",
   "metadata": {},
   "source": [
    "### Ans: In the cell below we train the perceptron on the binary label dataset with average word2vec vector representation build from our model for each review and then test it on the test set and display the results using the function we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "68c0810e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Our model, Perceptron has accuracy: 0.8092027768727601, precision: 0.777787737540337, recall: 0.8673097106302164, f-score: 0.8201129462914393\n"
     ]
    }
   ],
   "source": [
    "#training perceptron on our models average word2vec vector\n",
    "perc = Perceptron(max_iter = 75, eta0 = 0.005, random_state=65)\n",
    "perc.fit(X_train_binary, y_train_binary)\n",
    "\n",
    "ytest_ppn = perc.predict(X_test_binary)\n",
    "\n",
    "calc_aprf(\"Perceptron\",\"Our\",y_test_binary, ytest_ppn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e170a1",
   "metadata": {},
   "source": [
    "## Pre-trained word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def0caa",
   "metadata": {},
   "source": [
    "### Ans: In the cell below we train the perceptron on the binary label dataset with average word2vec vector representation build from the pre-trained model for each review and then test it on the test set and display the results using the function we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d470ed6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Pre-trained model, Perceptron has accuracy: 0.714518351496931, precision: 0.6459736655045188, recall: 0.9532467532467532, f-score: 0.770090591772088\n"
     ]
    }
   ],
   "source": [
    "#training perceptron on pretrained models average word2vec vector\n",
    "perc = Perceptron(max_iter = 75, eta0 = 0.005, random_state=65)\n",
    "perc.fit(X_train_binary_pre, y_train_binary_pre)\n",
    "\n",
    "ytest_ppn = perc.predict(X_test_binary_pre)\n",
    "\n",
    "calc_aprf(\"Perceptron\",\"Pre-trained\",y_test_binary_pre, ytest_ppn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4648eef7",
   "metadata": {},
   "source": [
    "## TF-IDF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903bd9e9",
   "metadata": {},
   "source": [
    "### Ans: In the cell below I display the results I got on the test dataset when TF-IDF Model was trained using perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a2c0387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For TF-IDF model, Perceptron has accuracy: 0.8655, precision: 0.8439, recall: 0.8964, f-score: 0.8694\n"
     ]
    }
   ],
   "source": [
    "print('For TF-IDF model, Perceptron has accuracy: 0.8655, precision: 0.8439, recall: 0.8964, f-score: 0.8694')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be73c64d",
   "metadata": {},
   "source": [
    "### Ans: Based on accuracy values on the testing dataset,from perceptron training, it seems that TF-IDF model performs the best, followed by the model we trained and then the pre-trained model. Thus TF-IDF is a more robust input feature, followed by our models' average word2vec representation and lastly the pretrained models average word2vec representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ce2c6a",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c684eb20",
   "metadata": {},
   "source": [
    "## Our word2vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66541cc7",
   "metadata": {},
   "source": [
    "### Ans: In the cell below we train the SVM on the binary label dataset with average word2vec vector representation build from our model for each review and then test it on the test set and display the results using the function we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ad7e6fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Our model, SVM has accuracy: 0.8573218716322899, precision: 0.8632027603003857, recall: 0.850217402169024, f-score: 0.8566608756955459\n"
     ]
    }
   ],
   "source": [
    "#training SVM on our models' average word2vec vector\n",
    "\n",
    "svc = SVC( max_iter = 1000, random_state=65)\n",
    "svc.fit(X_train_binary, y_train_binary)\n",
    "\n",
    "ytest_svm = svc.predict(X_test_binary)\n",
    "\n",
    "calc_aprf(\"SVM\",\"Our\",y_test_binary, ytest_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d976328",
   "metadata": {},
   "source": [
    "## Pre-trained word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab95670",
   "metadata": {},
   "source": [
    "### Ans: In the cell below we train the SVM on the binary label dataset with average word2vec vector representation build from the pre-trained model for each review and then test it on the test set and display the results using the function we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fa12cf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Pre-trained model, SVM has accuracy: 0.8177126393586371, precision: 0.8312538989394884, recall: 0.7987012987012987, f-score: 0.8146525371917668\n"
     ]
    }
   ],
   "source": [
    "#training SVM on pre-trained models' average word2vec vector\n",
    "\n",
    "svc = SVC( max_iter = 1000, random_state=65)\n",
    "svc.fit(X_train_binary_pre, y_train_binary_pre)\n",
    "\n",
    "ytest_svm = svc.predict(X_test_binary_pre)\n",
    "\n",
    "calc_aprf(\"SVM\",\"Pre-trained\",y_test_binary_pre, ytest_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212f4ec",
   "metadata": {},
   "source": [
    "## TF-IDF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c7980a",
   "metadata": {},
   "source": [
    "### Ans: In the cell below I display the results I got on the test dataset when TF-IDF Model was trained using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9f34d186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For TF-IDF model, SVM has accuracy: 0.8930, precision: 0.8914, recall: 0.8946, f-score: 0.8930\n"
     ]
    }
   ],
   "source": [
    "print('For TF-IDF model, SVM has accuracy: 0.8930, precision: 0.8914, recall: 0.8946, f-score: 0.8930')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17baa201",
   "metadata": {},
   "source": [
    "### Ans: Based on accuracy values on the testing dataset,from SVM training, it seems that TF-IDF model performs the best, followed by the model we trained and then the pre-trained model. Thus TF-IDF is a more robust input feature, followed by our models' average word2vec representation and lastly the pretrained models average word2vec representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91864bec",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d079705",
   "metadata": {},
   "source": [
    "### 4. Feedforward Neural Networks (25 points)\n",
    "### Using the features that you can generate using the models you prepared in the Word “Embedding section”, train a feedforward multilayer perceptron network for sentiment analysis classification. Consider a network with two hidden layers, each with 50 and 10 nodes, respectively. You can use cross entropy loss and your own choice for other hyperparamters, e.g., nonlinearity, number of epochs, etc. Part of getting good results is to select good values for these hyperparamters.\n",
    "### You can also refer to the following tutorial to familiarize yourself:\n",
    "https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist\n",
    "### Although the above tutorial is for image data but the concept of training an MLP is very similar to what we want to do.\n",
    "### (a) To generate the input features, use the average Word2Vec vectors similar to the “Simple models” section and train the neural network. Train a network for binary classification using class 1 and class 2 and also a ternary model for the three classes. Report accuracy values on the testing split for your MLP model for each of the binary and ternary classification cases.\n",
    "### (b) (To generate the input features, concatenate the first 10 Word2Vec vectors for each review as the input feature and train the neuralnetwork. Report the accuracy value on the testing split for your MLP model for each of the binary and ternary classification cases.\n",
    "### What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section (note you can compare the accuracy values for binary classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a9dc2",
   "metadata": {},
   "source": [
    "### In the cell below we first define the class MLP which initiatlizes the Multi-layer Perceptron for us, whos' input is average word2vec vector representation for each review. The init function of the class defines all the layers being used in the network and the forward function defines the structure of the MLP model. Similarly, the class MLP_vec initializes Multi-layer Perceptron for us, whos' input is first 10 word2vec vector representations for each review. \n",
    "\n",
    "### The TrainData class helps in iteratively feeding the data to the model while training. And, the TestData class helps in iteratively feeding the data to the model while testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ef47c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, classification = \"binary\", vocab_size = 300):\n",
    "        super(MLP, self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        if classification == \"binary\":\n",
    "            self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        else:\n",
    "            # For multi-classification\n",
    "            self.fc3 = nn.Linear(hidden_2, 4)\n",
    "            \n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.soft = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class MLP_vec(nn.Module):\n",
    "    def __init__(self, classification = \"binary\", vocab_size = 300):\n",
    "        super(MLP_vec, self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        if classification == \"binary\":\n",
    "            self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        else:\n",
    "            # For multi-classification\n",
    "            self.fc3 = nn.Linear(hidden_2, 4)\n",
    "        self.prod = 10\n",
    "        self.fc1 = nn.Linear(vocab_size * self.prod, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.soft = nn.Softmax(dim = 1)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1] * x.shape[2])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class trainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "class testData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, Y_data):\n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.Y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77200cc4",
   "metadata": {},
   "source": [
    "## Our word2vec Model (binary classification using average word2vec vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832aa8bc",
   "metadata": {},
   "source": [
    "### Part (a) Ans: In the cell below, we first initialize the model then choose its loss function and optimizer, then call the iterative training and testing data feeder functions from above. Afterwards we begin the training of the MLP network for the no of epochs given. And finally we evaluate the model based on the model saved after last epoch. The input of this network is the average word2vec vector generated by our model and output is either of the binary label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d60f265e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Our models average word2vec as input, for binary classification, MLP has a Accuracy: 0.8683241021528283, Precision: 0.8518624505174799, Recall: 0.8926483082612824, F1-Score: 0.8717786021085514\n"
     ]
    }
   ],
   "source": [
    "# Initializing MLP with average word2vec vector from our model as input for binary classification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "#device = torch.device('cuda')\n",
    "mlp_model = MLP() # binary classification\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(mlp_model.parameters(), lr=0.01) \n",
    "\n",
    "\n",
    "mlp_model = mlp_model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "training_data = trainData(torch.FloatTensor(X_train_binary), torch.LongTensor(y_train_binary))\n",
    "testing_data = testData(torch.FloatTensor(X_test_binary), torch.LongTensor(y_test_binary))\n",
    "\n",
    "train_loader = DataLoader(dataset = training_data, batch_size=16, shuffle = True)\n",
    "test_loader = DataLoader(dataset = testing_data, batch_size=16)\n",
    "\n",
    "#Training the Model\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "\n",
    "    mlp_model.train()\n",
    "    for input_data, label in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = mlp_model(input_data.to(device))\n",
    "        loss = criterion(output, label.to(device)) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * input_data.size(1)\n",
    "\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    #print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "    torch.save(mlp_model.state_dict(), 'mlp_model' + str(epoch + 1) + '.pt')\n",
    "\n",
    "\n",
    "\n",
    "# Evaluating the Model\n",
    "mlp_model.load_state_dict(torch.load('mlp_model' +str(n_epochs) + '.pt'))\n",
    "mlp_model = mlp_model.to('cpu')\n",
    "\n",
    "predictions, actual = list(), list()\n",
    "for test_data, test_label in test_loader:\n",
    "\n",
    "    pred = mlp_model(test_data.to('cpu'))\n",
    "    pred = pred.detach().numpy()\n",
    "    pred = argmax(pred, axis= 1)\n",
    "    target = test_label.numpy()\n",
    "    target = target.reshape((len(target), 1))\n",
    "    pred = pred.reshape((len(pred)), 1)\n",
    "    pred = pred.round()\n",
    "    predictions.append(pred)\n",
    "    actual.append(target)\n",
    "\n",
    "predictions, actual = vstack(predictions), vstack(actual)\n",
    "acc = accuracy_score(actual, predictions)\n",
    "prec = precision_score(actual, predictions)\n",
    "recall = recall_score(actual, predictions)\n",
    "f1 = f1_score(actual, predictions)\n",
    "print('Using Our models average word2vec as input, for binary classification, MLP has a Accuracy: {}, Precision: {}, Recall: {}, F1-Score: {}'.format(acc,prec,recall,f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cee0a7",
   "metadata": {},
   "source": [
    "## Pre-Trained word2vec Model (binary classification using average word2vec vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50087507",
   "metadata": {},
   "source": [
    "### Part (a) Ans: In the cell below, we first initialize the model then choose its loss function and optimizer, then call the iterative training and testing data feeder functions from above. Afterwards we begin the training of the MLP network for the no of epochs given. And finally we evaluate the model based on the model saved after last epoch. The input of this network is the average word2vec vector generated by the pre-trained model and output is either of the binary label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a607cf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pre-trained models average word2vec as input, for binary classification, MLP has a Accuracy: 0.8337467117624953, Precision: 0.8569828230022405, Recall: 0.8024475524475524, F1-Score: 0.8288190682556879\n"
     ]
    }
   ],
   "source": [
    "# Initializing MLP with average word2vec vector from pretrained model as input for binary classification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "#device = torch.device('cuda')\n",
    "mlp_model = MLP() # binary classification\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(mlp_model.parameters(), lr=0.01) \n",
    "\n",
    "mlp_model = mlp_model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "training_data = trainData(torch.FloatTensor(X_train_binary_pre), torch.LongTensor(y_train_binary_pre))\n",
    "testing_data = testData(torch.FloatTensor(X_test_binary_pre), torch.LongTensor(y_test_binary_pre))\n",
    "\n",
    "train_loader = DataLoader(dataset = training_data, batch_size=16, shuffle = True)\n",
    "test_loader = DataLoader(dataset = testing_data, batch_size=16)\n",
    "\n",
    "#Training the Model\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "\n",
    "    mlp_model.train()\n",
    "    for input_data, label in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = mlp_model(input_data.to(device))\n",
    "        loss = criterion(output, label.to(device)) #y_batch.unsqueeze(1) (label.unsqueeze(1)).to(device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * input_data.size(1)\n",
    "\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    #print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "    torch.save(mlp_model.state_dict(), 'mlp_model_pre' + str(epoch + 1) + '.pt')\n",
    "\n",
    "\n",
    "\n",
    "# Evaluating the Model\n",
    "\n",
    "mlp_model.load_state_dict(torch.load('mlp_model_pre' +str(n_epochs) + '.pt'))\n",
    "mlp_model = mlp_model.to('cpu')\n",
    "\n",
    "predictions, actual = list(), list()\n",
    "for test_data, test_label in test_loader:\n",
    "\n",
    "    pred = mlp_model(test_data.to('cpu'))\n",
    "    pred = pred.detach().numpy()\n",
    "    pred = argmax(pred, axis= 1)\n",
    "    target = test_label.numpy()\n",
    "    target = target.reshape((len(target), 1))\n",
    "    pred = pred.reshape((len(pred)), 1)\n",
    "    pred = pred.round()\n",
    "    predictions.append(pred)\n",
    "    actual.append(target)\n",
    "\n",
    "predictions, actual = vstack(predictions), vstack(actual)\n",
    "acc = accuracy_score(actual, predictions)\n",
    "prec = precision_score(actual, predictions)\n",
    "recall = recall_score(actual, predictions)\n",
    "f1 = f1_score(actual, predictions)\n",
    "print('Using Pre-trained models average word2vec as input, for binary classification, MLP has a Accuracy: {}, Precision: {}, Recall: {}, F1-Score: {}'.format(acc,prec,recall,f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a243eff3",
   "metadata": {},
   "source": [
    "## Our word2vec Model (Ternary classification using average word2vec vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7167289",
   "metadata": {},
   "source": [
    "### Part (a) Ans: In the cell below, we first initialize the model then choose its loss function and optimizer, then call the iterative training and testing data feeder functions from above. Afterwards we begin the training of the MLP network for the no of epochs given. And finally we evaluate the model based on the model saved after last epoch. The input of this network is the average word2vec vector generated by our model and output is anyone of the ternary label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "35fa59cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Our models average word2vec as input, for ternary classification, MLP has a Accuracy: 0.8319679318551922, Precision: 0.8605318745599306, Recall: 0.7936063936063936, F1-Score: 0.8257152508900033\n"
     ]
    }
   ],
   "source": [
    "# Initializing MLP with average word2vec vector from our model as input for ternary classification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "mlp_model = MLP(classification = \"multi-class\") \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(mlp_model.parameters(), lr=0.01) \n",
    "\n",
    "mlp_model = mlp_model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "training_data_multi = trainData(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "testing_data_multi = testData(torch.FloatTensor(X_test), torch.LongTensor(y_test))\n",
    "\n",
    "train_loader_multi = DataLoader(dataset = training_data_multi, batch_size=16, shuffle = True)\n",
    "test_loader_mutli = DataLoader(dataset = testing_data_multi, batch_size=16)\n",
    "\n",
    "#Training the Model\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "\n",
    "    mlp_model.train()\n",
    "    for input_data, label in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = mlp_model(input_data.to(device))\n",
    "        loss = criterion(output, label.to(device)) #y_batch.unsqueeze(1) (label.unsqueeze(1)).to(device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * input_data.size(1)\n",
    "\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    #print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "    torch.save(mlp_model.state_dict(), 'mlp_model_multi' + str(epoch + 1) + '.pt')\n",
    "\n",
    "\n",
    "# Evaluating the Model\n",
    "mlp_model.load_state_dict(torch.load('mlp_model_multi' +str(n_epochs) + '.pt'))\n",
    "mlp_model = mlp_model.to('cpu')\n",
    "\n",
    "predictions, actual = list(), list()\n",
    "for test_data, test_label in test_loader:\n",
    "\n",
    "    pred = mlp_model(test_data.to('cpu'))\n",
    "    pred = pred.detach().numpy()\n",
    "    pred = argmax(pred, axis= 1)\n",
    "    target = test_label.numpy()\n",
    "    target = target.reshape((len(target), 1))\n",
    "    pred = pred.reshape((len(pred)), 1)\n",
    "    pred = pred.round()\n",
    "    predictions.append(pred)\n",
    "    actual.append(target)\n",
    "\n",
    "predictions, actual = vstack(predictions), vstack(actual)\n",
    "acc = accuracy_score(actual, predictions)\n",
    "prec = precision_score(actual, predictions)\n",
    "recall = recall_score(actual, predictions)\n",
    "f1 = f1_score(actual, predictions)\n",
    "print('Using Our models average word2vec as input, for ternary classification, MLP has a Accuracy: {}, Precision: {}, Recall: {}, F1-Score: {}'.format(acc,prec,recall,f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f5c582",
   "metadata": {},
   "source": [
    "## Pre-trained word2vec Model (Ternary classification using average word2vec vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9530429f",
   "metadata": {},
   "source": [
    "### Part (a) Ans: In the cell below, we first initialize the model then choose its loss function and optimizer, then call the iterative training and testing data feeder functions from above. Afterwards we begin the training of the MLP network for the no of epochs given. And finally we evaluate the model based on the model saved after last epoch. The input of this network is the average word2vec vector generated by the pre-trained model and output is anyone of the ternary label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2bb585fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pre-trained models average word2vec as input, for ternary classification, MLP has a Accuracy: 0.826456219466366, Precision: 0.8746208869814021, Recall: 0.7634365634365634, F1-Score: 0.8152553673823176\n"
     ]
    }
   ],
   "source": [
    "# Initializing MLP with average word2vec vector from pre-trained model as input for ternary classification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "mlp_model = MLP(classification = \"multi-class\") \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(mlp_model.parameters(), lr=0.01) \n",
    "\n",
    "mlp_model = mlp_model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "training_data_multi = trainData(torch.FloatTensor(X_train_pre), torch.LongTensor(y_train_pre))\n",
    "testing_data_multi = testData(torch.FloatTensor(X_test_pre), torch.LongTensor(y_test_pre))\n",
    "\n",
    "train_loader_multi = DataLoader(dataset = training_data_multi, batch_size=16, shuffle = True)\n",
    "test_loader_mutli = DataLoader(dataset = testing_data_multi, batch_size=16)\n",
    "\n",
    "#Training the Model\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "\n",
    "    mlp_model.train()\n",
    "    for input_data, label in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = mlp_model(input_data.to(device))\n",
    "        loss = criterion(output, label.to(device)) #y_batch.unsqueeze(1) (label.unsqueeze(1)).to(device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * input_data.size(1)\n",
    "\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    #print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "    torch.save(mlp_model.state_dict(), 'mlp_model_multi_pre' + str(epoch + 1) + '.pt')\n",
    "\n",
    "\n",
    "# Evaluating the Model\n",
    "\n",
    "mlp_model.load_state_dict(torch.load('mlp_model_multi_pre' +str(n_epochs) + '.pt'))\n",
    "mlp_model = mlp_model.to('cpu')\n",
    "\n",
    "predictions, actual = list(), list()\n",
    "for test_data, test_label in test_loader:\n",
    "\n",
    "    pred = mlp_model(test_data.to('cpu'))\n",
    "    pred = pred.detach().numpy()\n",
    "    pred = argmax(pred, axis= 1)\n",
    "    target = test_label.numpy()\n",
    "    target = target.reshape((len(target), 1))\n",
    "    pred = pred.reshape((len(pred)), 1)\n",
    "    pred = pred.round()\n",
    "    predictions.append(pred)\n",
    "    actual.append(target)\n",
    "\n",
    "predictions, actual = vstack(predictions), vstack(actual)\n",
    "\n",
    "acc = accuracy_score(actual, predictions)\n",
    "prec = precision_score(actual, predictions)\n",
    "recall = recall_score(actual, predictions)\n",
    "f1 = f1_score(actual, predictions)\n",
    "print('Using Pre-trained models average word2vec as input, for ternary classification, MLP has a Accuracy: {}, Precision: {}, Recall: {}, F1-Score: {}'.format(acc,prec,recall,f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993f4ba5",
   "metadata": {},
   "source": [
    "## Our word2vec Model (binary classification using first 10 word2vec vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6edf17",
   "metadata": {},
   "source": [
    "### Part (b) Ans: In the cell below, we first initialize the model then choose its loss function and optimizer, then call the iterative training and testing data feeder functions from above. Afterwards we begin the training of the MLP network for the no of epochs given. And finally we evaluate the model based on the model saved after last epoch. The input of this network is the first 10 word2vec vector generated for each review, by our model and output is either of the binary label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "81eb1fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Our models first 10 word2vec vector for each review as input, for binary classification, MLP has a Accuracy: 0.7515258909234102, Precision: 0.7474842539636575, Recall: 0.7165660351169408, F1-Score: 0.7316986747927149\n"
     ]
    }
   ],
   "source": [
    "# Initializing MLP, with first 10 word2vec vector for each review, from our model as input for binary classification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "#device = torch.device('cuda')\n",
    "mlp_model = MLP_vec() # binary classification\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(mlp_model.parameters(), lr=0.01) \n",
    "\n",
    "mlp_model = mlp_model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "training_data = trainData(torch.FloatTensor(X_train_binary_10), torch.LongTensor(y_train_binary_10))\n",
    "testing_data = testData(torch.FloatTensor(X_test_binary_10), torch.LongTensor(y_test_binary_10))\n",
    "\n",
    "train_loader = DataLoader(dataset = training_data, batch_size=16, shuffle = True)\n",
    "test_loader = DataLoader(dataset = testing_data, batch_size=16)\n",
    "\n",
    "#Training the Model\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "\n",
    "    mlp_model.train()\n",
    "    for input_data, label in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = mlp_model(input_data.to(device))\n",
    "        loss = criterion(output, label.to(device)) #y_batch.unsqueeze(1) (label.unsqueeze(1)).to(device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * input_data.size(1)\n",
    "\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    #print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "    torch.save(mlp_model.state_dict(), 'mlp_model_vec' + str(epoch + 1) + '.pt')\n",
    "\n",
    "\n",
    "\n",
    "# Evaluating the Model\n",
    "\n",
    "mlp_model.load_state_dict(torch.load('mlp_model_vec' +str(n_epochs) + '.pt'))\n",
    "mlp_model = mlp_model.to('cpu')\n",
    "\n",
    "predictions, actual = list(), list()\n",
    "for test_data, test_label in test_loader:\n",
    "\n",
    "    pred = mlp_model(test_data.to('cpu'))\n",
    "    pred = pred.detach().numpy()\n",
    "    pred = argmax(pred, axis= 1)\n",
    "    target = test_label.numpy()\n",
    "    target = target.reshape((len(target), 1))\n",
    "    pred = pred.reshape((len(pred)), 1)\n",
    "    pred = pred.round()\n",
    "    predictions.append(pred)\n",
    "    actual.append(target)\n",
    "\n",
    "predictions, actual = vstack(predictions), vstack(actual)\n",
    "acc = accuracy_score(actual, predictions)\n",
    "prec = precision_score(actual, predictions)\n",
    "recall = recall_score(actual, predictions)\n",
    "f1 = f1_score(actual, predictions)\n",
    "print('Using Our models first 10 word2vec vector for each review as input, for binary classification, MLP has a Accuracy: {}, Precision: {}, Recall: {}, F1-Score: {}'.format(acc,prec,recall,f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f7f1c",
   "metadata": {},
   "source": [
    "## Pre-trained word2vec Model (binary classification using first 10 word2vec vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf8c7f",
   "metadata": {},
   "source": [
    "### Part (b) Ans: In the cell below, we first initialize the model then choose its loss function and optimizer, then call the iterative training and testing data feeder functions from above. Afterwards we begin the training of the MLP network for the no of epochs given. And finally we evaluate the model based on the model saved after last epoch. The input of this network is the first 10 word2vec vector generated for each review, by the pre-trained model and output is either of the binary label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "74df715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pre-trained models first 10 word2vec vector for each review as input, for binary classification, MLP has a Accuracy: 0.7365906338973067, Precision: 0.7573954983922829, Recall: 0.6519061786480316, F1-Score: 0.7007027851113672\n"
     ]
    }
   ],
   "source": [
    "# Initializing MLP, with first 10 word2vec vector for each review, from pre-trained model as input for binary classification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "#device = torch.device('cuda')\n",
    "mlp_model = MLP_vec() # binary classification\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(mlp_model.parameters(), lr=0.01) \n",
    "\n",
    "mlp_model = mlp_model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "training_data = trainData(torch.FloatTensor(X_train_binary_pre_10), torch.LongTensor(y_train_binary_pre_10))\n",
    "testing_data = testData(torch.FloatTensor(X_test_binary_pre_10), torch.LongTensor(y_test_binary_pre_10))\n",
    "\n",
    "train_loader = DataLoader(dataset = training_data, batch_size=16, shuffle = True)\n",
    "test_loader = DataLoader(dataset = testing_data, batch_size=16)\n",
    "\n",
    "#Training the model\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "\n",
    "    mlp_model.train()\n",
    "    for input_data, label in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = mlp_model(input_data.to(device))\n",
    "        loss = criterion(output, label.to(device)) #y_batch.unsqueeze(1) (label.unsqueeze(1)).to(device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * input_data.size(1)\n",
    "\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    #print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "    torch.save(mlp_model.state_dict(), 'mlp_model_vec_pre' + str(epoch + 1) + '.pt')\n",
    "\n",
    "\n",
    "\n",
    "# Evaluating the Model\n",
    "\n",
    "mlp_model.load_state_dict(torch.load('mlp_model_vec_pre' +str(n_epochs) + '.pt'))\n",
    "mlp_model = mlp_model.to('cpu')\n",
    "\n",
    "predictions, actual = list(), list()\n",
    "for test_data, test_label in test_loader:\n",
    "\n",
    "    pred = mlp_model(test_data.to('cpu'))\n",
    "    pred = pred.detach().numpy()\n",
    "    pred = argmax(pred, axis= 1)\n",
    "    target = test_label.numpy()\n",
    "    target = target.reshape((len(target), 1))\n",
    "    pred = pred.reshape((len(pred)), 1)\n",
    "    pred = pred.round()\n",
    "    predictions.append(pred)\n",
    "    actual.append(target)\n",
    "\n",
    "predictions, actual = vstack(predictions), vstack(actual)\n",
    "acc = accuracy_score(actual, predictions)\n",
    "prec = precision_score(actual, predictions)\n",
    "recall = recall_score(actual, predictions)\n",
    "f1 = f1_score(actual, predictions)\n",
    "print('Using Pre-trained models first 10 word2vec vector for each review as input, for binary classification, MLP has a Accuracy: {}, Precision: {}, Recall: {}, F1-Score: {}'.format(acc,prec,recall,f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3fc45d",
   "metadata": {},
   "source": [
    "## Our word2vec Model (ternary classification using first 10 word2vec vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0262c",
   "metadata": {},
   "source": [
    "### Part (b) Ans: In the cell below, we first initialize the model then choose its loss function and optimizer, then call the iterative training and testing data feeder functions from above. Afterwards we begin the training of the MLP network for the no of epochs given. And finally we evaluate the model based on the model saved after last epoch. The input of this network is the first 10 word2vec vector generated for each review, by our model and output is anyone of the ternary label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "62b77763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Our models first 10 word2vec vector for each review as input, for ternary classification, MLP has a Accuracy: 0.7335144156821678, Precision: 0.696965913347484, Recall: 0.7724347886251989, F1-Score: 0.7327622985789767\n"
     ]
    }
   ],
   "source": [
    "# Initializing MLP, with first 10 word2vec vector for each review, from our model as input for ternary classification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "mlp_model = MLP_vec(classification = \"multi-class\") \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(mlp_model.parameters(), lr=0.01) \n",
    "\n",
    "mlp_model = mlp_model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "training_data_multi = trainData(torch.FloatTensor(X_train_10), torch.LongTensor(y_train_10))\n",
    "testing_data_multi = testData(torch.FloatTensor(X_test_10), torch.LongTensor(y_test_10))\n",
    "\n",
    "train_loader_multi = DataLoader(dataset = training_data_multi, batch_size=16, shuffle = True)\n",
    "test_loader_mutli = DataLoader(dataset = testing_data_multi, batch_size=16)\n",
    "\n",
    "#Training the model\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "\n",
    "    mlp_model.train()\n",
    "    for input_data, label in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = mlp_model(input_data.to(device))\n",
    "        loss = criterion(output, label.to(device)) #y_batch.unsqueeze(1) (label.unsqueeze(1)).to(device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * input_data.size(1)\n",
    "\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    #print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "    torch.save(mlp_model.state_dict(), 'mlp_model_multi_vec' + str(epoch + 1) + '.pt')\n",
    "\n",
    "\n",
    "# Evaluating the Model\n",
    "mlp_model.load_state_dict(torch.load('mlp_model_multi_vec' +str(n_epochs) + '.pt'))\n",
    "mlp_model = mlp_model.to('cpu')\n",
    "\n",
    "predictions, actual = list(), list()\n",
    "for test_data, test_label in test_loader:\n",
    "\n",
    "    pred = mlp_model(test_data.to('cpu'))\n",
    "    pred = pred.detach().numpy()\n",
    "    pred = argmax(pred, axis= 1)\n",
    "    target = test_label.numpy()\n",
    "    target = target.reshape((len(target), 1))\n",
    "    pred = pred.reshape((len(pred)), 1)\n",
    "    pred = pred.round()\n",
    "    predictions.append(pred)\n",
    "    actual.append(target)\n",
    "\n",
    "predictions, actual = vstack(predictions), vstack(actual)\n",
    "acc = accuracy_score(actual, predictions)\n",
    "prec = precision_score(actual, predictions)\n",
    "recall = recall_score(actual, predictions)\n",
    "f1 = f1_score(actual, predictions)\n",
    "print('Using Our models first 10 word2vec vector for each review as input, for ternary classification, MLP has a Accuracy: {}, Precision: {}, Recall: {}, F1-Score: {}'.format(acc,prec,recall,f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5460d47a",
   "metadata": {},
   "source": [
    "## Pre-trained word2vec Model (ternary classification using first 10 word2vec vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff51606",
   "metadata": {},
   "source": [
    "### Part (b) Ans: In the cell below, we first initialize the model then choose its loss function and optimizer, then call the iterative training and testing data feeder functions from above. Afterwards we begin the training of the MLP network for the no of epochs given. And finally we evaluate the model based on the model saved after last epoch. The input of this network is the first 10 word2vec vector generated for each review, by the pre-trained model and output is anyone of the ternary label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8593b752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pre-trained models first 10 word2vec vector for each review as input, for ternary classification, MLP has a Accuracy: 0.7347907189841935, Precision: 0.7669217186580342, Recall: 0.6310800525842386, F1-Score: 0.6924011235102103\n"
     ]
    }
   ],
   "source": [
    "# Initializing MLP, with first 10 word2vec vector for each review, from pre-trained model as input for ternary classification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "mlp_model = MLP_vec(classification = \"multi-class\") \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(mlp_model.parameters(), lr=0.01) \n",
    "\n",
    "mlp_model = mlp_model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "training_data_multi = trainData(torch.FloatTensor(X_train_pre_10), torch.LongTensor(y_train_pre_10))\n",
    "testing_data_multi = testData(torch.FloatTensor(X_test_pre_10), torch.LongTensor(y_test_pre_10))\n",
    "\n",
    "train_loader_multi = DataLoader(dataset = training_data_multi, batch_size=16, shuffle = True)\n",
    "test_loader_mutli = DataLoader(dataset = testing_data_multi, batch_size=16)\n",
    "\n",
    "#Training the model\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "\n",
    "    mlp_model.train()\n",
    "    for input_data, label in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = mlp_model(input_data.to(device))\n",
    "        loss = criterion(output, label.to(device)) #y_batch.unsqueeze(1) (label.unsqueeze(1)).to(device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * input_data.size(1)\n",
    "\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    #print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "    torch.save(mlp_model.state_dict(), 'mlp_model_multi_vec_pre' + str(epoch + 1) + '.pt')\n",
    "\n",
    "\n",
    "# Evaluating the Model\n",
    "mlp_model.load_state_dict(torch.load('mlp_model_multi_vec_pre' +str(n_epochs) + '.pt'))\n",
    "mlp_model = mlp_model.to('cpu')\n",
    "\n",
    "predictions, actual = list(), list()\n",
    "for test_data, test_label in test_loader:\n",
    "\n",
    "    pred = mlp_model(test_data.to('cpu'))\n",
    "    pred = pred.detach().numpy()\n",
    "    pred = argmax(pred, axis= 1)\n",
    "    target = test_label.numpy()\n",
    "    target = target.reshape((len(target), 1))\n",
    "    pred = pred.reshape((len(pred)), 1)\n",
    "    pred = pred.round()\n",
    "    predictions.append(pred)\n",
    "    actual.append(target)\n",
    "\n",
    "predictions, actual = vstack(predictions), vstack(actual)\n",
    "acc = accuracy_score(actual, predictions)\n",
    "prec = precision_score(actual, predictions)\n",
    "recall = recall_score(actual, predictions)\n",
    "f1 = f1_score(actual, predictions)\n",
    "print('Using Pre-trained models first 10 word2vec vector for each review as input, for ternary classification, MLP has a Accuracy: {}, Precision: {}, Recall: {}, F1-Score: {}'.format(acc,prec,recall,f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1016a05",
   "metadata": {},
   "source": [
    "### Ans: By comparing the accuracy values obtained for binary classification using average word2vec vector representation for each review, its clear that Multilayer Perceptron performs better than SVM and perceptron for both cases, when input is generated by our word2vec model and when input is generated by pre-trained word2vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75af2a73",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4420c1",
   "metadata": {},
   "source": [
    "### 5. Recurrent Neural Networks (20 points)\n",
    "### Using the features that you can generate using the models you prepared in the “Word Embedding” section, train a recurrent neural network (RNN) for sentiment analysis classification. You can refer to the following tutorial to familiarize yourself:\n",
    "   https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "### (a) Train a simple RNN for sentiment analysis. You can consider an RNN cell with the hidden state size of 50. To feed your data into our RNN, limit the maximum review length to 50 by truncating longer reviews and padding shorter reviews with a null value (0). Train the RNN network for binary classification using class 1 and class 2 and also a ternary model for the three classes. Report accuracy values on the testing split for your RNN model.\n",
    "### (b) Repeat part (a) by considering a gated recurrent unit cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6329a593",
   "metadata": {},
   "source": [
    "### In the cell below we first define the class rnnModel which initiatlizes the RNN network for us, whos' input is the first 50 word2vec vector representation for each review ( if a review has less than 50 representations then we use padding). The init function of the class defines all the layers being used in the network and the forward function defines the structure of the RNN model. Similarly, the class gruModel  initializes the GRU network for us, whos' input is the first 50 word2vec vector representation for each review ( if a review has less than 50 representations then we use padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "26556ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnnModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, model_type = \"rnn\"):\n",
    "        super(rnnModel, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.model_type = model_type\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.layer = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(2500, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.layer(x, hidden)\n",
    "\n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, out.shape[1] * out.shape[2])\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)#.cuda()\n",
    "        return hidden\n",
    "    \n",
    "class gruModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, model_type = \"gru\"):\n",
    "        super(gruModel, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.model_type = model_type\n",
    "\n",
    "        #Defining the layers\n",
    "        # GRU Layer\n",
    "        self.layer = nn.GRU(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(2500, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.layer(x, hidden)\n",
    "\n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        #out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = out.contiguous().view(-1, out.shape[1] * out.shape[2])\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)#.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef77e01",
   "metadata": {},
   "source": [
    "### In the cell below rnn_train function is used to train any given RNN or GRU network for the no of epochs provided with the input features and output features provided. Similarly, rnn_test function is used to test the RNN or GRU model provided, for the epoch no provided, on the input and output features provided. The my_collate function is used to train the rnn or gru model using small batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c2d9762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    target = [item[1] for item in batch]\n",
    "    \n",
    "    return data, target\n",
    "\n",
    "#used for training rnn or gru network\n",
    "def rnn_train(model, epoch, dataset_x, dataset_y, name):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    rnn_train = RNN_Data(dataset_x, dataset_y)\n",
    "    train_loader_mode = DataLoader(dataset = rnn_train, batch_size=8, shuffle = True, collate_fn=my_collate, drop_last=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    for ep in range(1, epoch + 1):\n",
    "\n",
    "        for input_data, label in train_loader_mode:\n",
    "            optimizer.zero_grad()\n",
    "            input_data = torch.stack(input_data)\n",
    "            #print(input_data.shape)\n",
    "            label = torch.stack(label)\n",
    "            #print(label.shape)\n",
    "            output, hidden = model(input_data.to(device))\n",
    "            #print(output.shape)\n",
    "            #output = torch.tensor(output, dtype=torch.long)\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "            loss = criterion(output,label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        #print('Epoch: {} \\tTraining Loss: {:.6f}'.format(ep, loss.item()))\n",
    "        torch.save(model.state_dict(), name + str(ep) + '.pt')\n",
    "\n",
    "#used for testing rnn or gru network\n",
    "def rnn_test(model, epoch, dataset_x, dataset_y, name,model_name,w2v_name,classify):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    rnn_test = RNN_Data(dataset_x, dataset_y)\n",
    "    test_loader_mode = DataLoader(dataset = rnn_test, batch_size=8, collate_fn=my_collate, drop_last=True)\n",
    "\n",
    "\n",
    "    model.load_state_dict(torch.load(name +str(epoch) + '.pt'))\n",
    "    model = model.to(device)\n",
    "\n",
    "    predictions, actual = list(), list()\n",
    "    for test_data, test_label in test_loader_mode:\n",
    "        test_data = torch.stack(test_data)\n",
    "        test_label = torch.stack(test_label)\n",
    "        pred, hid = model(test_data.to('cpu'))\n",
    "        pred = pred.to('cpu')\n",
    "        pred = pred.detach().numpy()\n",
    "        pred = argmax(pred, axis= 1)\n",
    "        target = test_label.numpy()\n",
    "        target = target.reshape((len(target), 1))\n",
    "        pred = pred.reshape((len(pred)), 1)\n",
    "        pred = pred.round()\n",
    "        predictions.append(pred)\n",
    "        actual.append(target)\n",
    "\n",
    "    predictions, actual = vstack(predictions), vstack(actual)\n",
    "    acc = accuracy_score(actual, predictions)\n",
    "    print('Using {} models first 50 word2vec vector for each review as input, for {} classification, {} has a Accuracy: {}'.format(w2v_name,classify,model_name,acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecfd6da",
   "metadata": {},
   "source": [
    "### The RNN_Data class defined in the cell below, helps iteratively feed the data to the RNN or GRU model while training or testing. Note: when any input data is called which has less then 50 word vectors the remaining word vectors are padded with zeroes in the _getitem_ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "edf598fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Data(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, Y_data):\n",
    "        \n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.X_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        pad = np.zeros((50, 300), dtype = float)\n",
    "        pad[-len(self.X_data[index]):] = np.array(self.X_data[index])#[:50]\n",
    "        X = torch.FloatTensor(pad)\n",
    "        Y = torch.tensor(self.Y_data[index])\n",
    "            \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d341f4",
   "metadata": {},
   "source": [
    "### Our word2vec model (binary classification using first 50 word2vec vectors on RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79ceb02",
   "metadata": {},
   "source": [
    "### Part (a) Ans: In the cell below we first construct our RNN model by calling the rnnModel class. Then we use the rnn_train function to train that model. Afterwards we use the rnn_test function to test the model saved after the last epoch on the testing set. The input of this network is the first 50 word2vec vector generated for each review, by our model and output is anyone of the binary label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "13cd5f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Our models first 50 word2vec vector for each review as input, for Binary classification, RNN has a Accuracy: 0.8583316623220373\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "rnn = rnnModel(300, 3, 50, 1)\n",
    "rnn = rnn.to(device)\n",
    "\n",
    "rnn_train(rnn, 5, X_train_binary_50, y_train_binary_50, name = \"rnn_binary_model\")\n",
    "rnn_test(rnn, 5, X_test_binary_50, y_test_binary_50, name = \"rnn_binary_model\", model_name = \"RNN\", w2v_name=\"Our\", classify=\"Binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5610182",
   "metadata": {},
   "source": [
    "### Pre-trained word2vec model (binary classification using first 50 word2vec vectors on RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34879b46",
   "metadata": {},
   "source": [
    "### Part (a) Ans: In the cell below we first construct our RNN model by calling the rnnModel class. Then we use the rnn_train function to train that model. Afterwards we use the rnn_test function to test the model saved after the last epoch on the testing set. The input of this network is the first 50 word2vec vector generated for each review, by the pre-trained model and output is anyone of the binary label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2dcbb795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pre-trained models first 50 word2vec vector for each review as input, for Binary classification, RNN has a Accuracy: 0.8421026257767088\n"
     ]
    }
   ],
   "source": [
    "rnn = rnnModel(300, 3, 50, 1)\n",
    "rnn = rnn.to(device)\n",
    "\n",
    "rnn_train(rnn, 5, X_train_binary_pre_50, y_train_binary_pre_50, name = \"rnn_pre_binary_model\")\n",
    "rnn_test(rnn, 5, X_test_binary_pre_50, y_test_binary_pre_50, name = \"rnn_pre_binary_model\", model_name = \"RNN\", w2v_name=\"Pre-trained\", classify=\"Binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8155be7d",
   "metadata": {},
   "source": [
    "### Our word2vec model (ternary classification using first 50 word2vec vectors on RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f0582",
   "metadata": {},
   "source": [
    "### Part (a) Ans: In the cell below we first construct our RNN model by calling the rnnModel class. Then we use the rnn_train function to train that model. Afterwards we use the rnn_test function to test the model saved after the last epoch on the testing set. The input of this network is the first 50 word2vec vector generated for each review, by our model and output is anyone of the ternary label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4c736b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Our models first 50 word2vec vector for each review as input, for Ternary classification, RNN has a Accuracy: 0.6977247236019869\n"
     ]
    }
   ],
   "source": [
    "rnn = rnnModel(300, 4, 50, 1)\n",
    "rnn = rnn.to(device)\n",
    "\n",
    "rnn_train(rnn, 5, X_train_50, y_train_50, name = \"rnn_multi_model\")\n",
    "rnn_test(rnn, 5, X_test_50, y_test_50, name = \"rnn_multi_model\",  model_name = \"RNN\", w2v_name=\"Our\", classify=\"Ternary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c32d73a",
   "metadata": {},
   "source": [
    "### Pre-trained word2vec model (ternary classification using first 50 word2vec vectors on RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34109af5",
   "metadata": {},
   "source": [
    "### Part (a) Ans: In the cell below we first construct our RNN model by calling the rnnModel class. Then we use the rnn_train function to train that model. Afterwards we use the rnn_test function to test the model saved after the last epoch on the testing set. The input of this network is the first 50 word2vec vector generated for each review, by the pre-trained model and output is anyone of the ternary label¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "fe8f7165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pre-trained models first 50 word2vec vector for each review as input, for Ternary classification, RNN has a Accuracy: 0.6671604740550929\n"
     ]
    }
   ],
   "source": [
    "rnn = rnnModel(300, 4, 50, 1)\n",
    "rnn = rnn.to(device)\n",
    "\n",
    "rnn_train(rnn, 5, X_train_pre_50, y_train_pre_50, name = \"rnn_pre_multi_model\")\n",
    "rnn_test(rnn, 5, X_test_pre_50, y_test_pre_50, name = \"rnn_pre_multi_model\", model_name = \"RNN\", w2v_name=\"Pre-trained\", classify=\"Ternary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2439a70f",
   "metadata": {},
   "source": [
    "### Our word2vec model (binary classification using first 50 word2vec vectors on GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa21c32",
   "metadata": {},
   "source": [
    "### Part (b) Ans: In the cell below we first construct our GRU model by calling the gruModel class. Then we use the rnn_train function to train that model. Afterwards we use the rnn_test function to test the model saved after the last epoch on the testing set. The input of this network is the first 50 word2vec vector generated for each review, by our model and output is anyone of the binary label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "50fb3d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Our models first 50 word2vec vector for each review as input, for Binary classification, GRU has a Accuracy: 0.8769801483858031\n"
     ]
    }
   ],
   "source": [
    "gru_model = gruModel(300, 3, 50, 1, model_type=\"gru\")\n",
    "gru_model = gru_model.to(device)\n",
    "\n",
    "rnn_train(gru_model, 5, X_train_binary_50, y_train_binary_50, name = \"gru_binary_model\")\n",
    "rnn_test(gru_model, 5, X_test_binary_50, y_test_binary_50, name = \"gru_binary_model\", model_name = \"GRU\", w2v_name=\"Our\", classify=\"Binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd83eba",
   "metadata": {},
   "source": [
    "### Pre-trained word2vec model (binary classification using first 50 word2vec vectors on GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f84720",
   "metadata": {},
   "source": [
    "### Part (b) Ans: In the cell below we first construct our GRU model by calling the gruModel class. Then we use the rnn_train function to train that model. Afterwards we use the rnn_test function to test the model saved after the last epoch on the testing set. The input of this network is the first 50 word2vec vector generated for each review, by the pre-trained model and output is anyone of the binary label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e9395bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pre-trained models first 50 word2vec vector for each review as input, for Binary classification, GRU has a Accuracy: 0.8452345159350572\n"
     ]
    }
   ],
   "source": [
    "gru_model = gruModel(300, 3, 50, 1, model_type=\"gru\")\n",
    "gru_model = gru_model.to(device)\n",
    "\n",
    "rnn_train(gru_model, 5, X_train_binary_pre_50, y_train_binary_pre_50, name = \"gru_pre_binary_model\")\n",
    "rnn_test(gru_model, 5, X_test_binary_pre_50, y_test_binary_pre_50, name = \"gru_pre_binary_model\", model_name = \"GRU\", w2v_name=\"Pre-trained\", classify=\"Binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622ed9ae",
   "metadata": {},
   "source": [
    "### Our word2vec model (ternary classification using first 50 word2vec vectors on GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805569f2",
   "metadata": {},
   "source": [
    "### Part (b) Ans: In the cell below we first construct our GRU model by calling the gruModel class. Then we use the rnn_train function to train that model. Afterwards we use the rnn_test function to test the model saved after the last epoch on the testing set. The input of this network is the first 50 word2vec vector generated for each review, by our model and output is anyone of the ternary label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f47b4d38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Our models first 50 word2vec vector for each review as input, for Ternary classification, GRU has a Accuracy: 0.6985058484217272\n"
     ]
    }
   ],
   "source": [
    "gru_model = gruModel(300, 4, 50, 1, model_type=\"gru\")\n",
    "gru_model = gru_model.to(device)\n",
    "\n",
    "rnn_train(gru_model, 5, X_train_50, y_train_50, name = \"gru_multi_model\")\n",
    "rnn_test(gru_model, 5, X_test_50, y_test_50, name = \"gru_multi_model\", model_name = \"GRU\", w2v_name=\"Our\", classify=\"Ternary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a25d009",
   "metadata": {},
   "source": [
    "### Pre-trained word2vec model (ternary classification using first 50 word2vec vectors on GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7777fb2c",
   "metadata": {},
   "source": [
    "### Part (b) Ans: In the cell below we first construct our GRU model by calling the gruModel class. Then we use the rnn_train function to train that model. Afterwards we use the rnn_test function to test the model saved after the last epoch on the testing set. The input of this network is the first 50 word2vec vector generated for each review, by the pre-trained model and output is anyone of the ternary label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6a1540a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Pre-trained models first 50 word2vec vector for each review as input, for Ternary classification, GRU has a Accuracy: 0.6803331197950032\n"
     ]
    }
   ],
   "source": [
    "gru_model = gruModel(300, 4, 50, 1, model_type=\"gru\")\n",
    "gru_model = gru_model.to(device)\n",
    "\n",
    "rnn_train(gru_model, 5, X_train_pre_50, y_train_pre_50, name = \"gru_pre_multi_model\")\n",
    "rnn_test(gru_model, 5, X_test_pre_50, y_test_pre_50, name = \"gru_pre_multi_model\", model_name = \"GRU\", w2v_name=\"Pre-trained\", classify=\"Ternary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda04309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
